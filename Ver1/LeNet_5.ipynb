{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1dwysN6WLzZo"
      },
      "outputs": [],
      "source": [
        "import pickle as pk\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LO3e7v8TDwwk"
      },
      "outputs": [],
      "source": [
        "class utils():\n",
        "    def padder(self, data, pad_length):\n",
        "        im = data.copy()\n",
        "        m = im.shape[0]\n",
        "        n = im.shape[1]\n",
        "        M, N = m + 2*pad_length, n + 2*pad_length \n",
        "        im2 = np.zeros((M,N))\n",
        "        im2[pad_length:m+pad_length , pad_length:n+pad_length] = im[:,:]\n",
        "        \n",
        "        return im2\n",
        "    \n",
        "    def Conv2D(self, inputs, weights, bias, padding, K, F, stride =1):\n",
        "        C, W, H = inputs.shape\n",
        "        WW = (W - K)//stride + 1\n",
        "        HH = (H - K)//stride + 1\n",
        "\n",
        "        feature_maps = np.zeros((F, WW, HH))\n",
        "\n",
        "        for f in range(F):\n",
        "            for w in range(WW):\n",
        "                for h in range(HH):\n",
        "                    # ic(f, w, h, K, weights[f, :, :, :].shape, bias[f].shape, inputs[:, w:w+K, h:h+K].shape)\n",
        "                    wi = w * stride\n",
        "                    wj = wi + K \n",
        "                    hi = h * stride\n",
        "                    hj = hi + K \n",
        "                    feature_maps[f,wi,hi]=np.sum(inputs[:,wi:wj,hi:hj]*weights[f,:,:,:])+bias[f]\n",
        "\n",
        "        return feature_maps\n",
        "\n",
        "    def AvgPool(self, data, pool_size, stride):\n",
        "        C, W, H = data.shape\n",
        "\n",
        "        new_width = (W - pool_size)//stride + 1\n",
        "        new_height = (H - pool_size)//stride + 1\n",
        "\n",
        "        out = np.zeros((C, new_width, new_height))\n",
        "\n",
        "        for c in range(C):\n",
        "            for w in range(new_width):\n",
        "                for h in range(new_height):\n",
        "                    out[c, w, h] = np.mean(data[c, w*stride:w*stride+pool_size, h*stride:h*stride+pool_size])\n",
        "  \n",
        "        return out\n",
        "\n",
        "    def softmax(self, x):\n",
        "        denom = np.sum(np.exp(x))\n",
        "        y = np.exp(x)/denom;\n",
        "        return y\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def Tanh(self, x):\n",
        "        a = 1.7159 \n",
        "        s = 2/3\n",
        "        return a*np.tanh(s*x)\n",
        "\n",
        "    def backTanh(self, x):\n",
        "        a = 1.7159 \n",
        "        s = 2/3\n",
        "        return a*s*(1-np.tanh(s*x)**2)\n",
        "    \n",
        "    def ReLU(self, x):\n",
        "        return np.max(x,0)\n",
        "    \n",
        "    def normal(self, data):\n",
        "        padded = self.padder(data,2)\n",
        "    \n",
        "    def vanilla(self, data, weights, bias, activation = 'Tanh'):\n",
        "        if activation == 'Tanh':\n",
        "            # ic(np.transpose(weights).shape, data[:,0].shape)\n",
        "            # ic(data)\n",
        "            return self.Tanh(np.dot(np.transpose(weights),data) + bias)\n",
        "        elif activation == 'sigmoid':\n",
        "            return  self.sigmoid(np.dot(np.transpose(weights),data) + bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZaJ9dDTNqyEB"
      },
      "outputs": [],
      "source": [
        "util = utils()\n",
        "class Layers():\n",
        "    class CONV():\n",
        "        def __init__(self, inputs_channel, num_filters, kernel_size):\n",
        "          self.F = num_filters\n",
        "          self.K = kernel_size\n",
        "          self.C = inputs_channel\n",
        "\n",
        "          self.weights = 2*(np.random.rand(self.F,self.C, self.K, self.K)-0.5)\n",
        "          self.bias = 2*(np.random.rand(self.F, 1)-0.5)\n",
        "\n",
        "        def forward(self, inputs, padding, stride):\n",
        "            C = inputs.shape[0]\n",
        "            W = inputs.shape[1]+2*padding\n",
        "            H = inputs.shape[2]+2*padding\n",
        "\n",
        "            self.inputs = np.zeros((C, W, H))\n",
        "\n",
        "            for c in range(inputs.shape[0]):\n",
        "                self.inputs[c,:,:] = util.padder(inputs[c,:,:], padding)\n",
        "\n",
        "            return util.Conv2D(self.inputs, self.weights, self.bias, padding, self.K, self.F, stride)\n",
        "          \n",
        "        def backward(self, dy, stride, learning_rate):\n",
        "            dy = util.backTanh(dy)\n",
        "\n",
        "            C, W, H = self.inputs.shape\n",
        "\n",
        "            dx = np.zeros(self.inputs.shape)\n",
        "            dw = np.zeros(self.weights.shape)\n",
        "            db = np.zeros(self.bias.shape)\n",
        "\n",
        "            if len(dy.shape)==2:\n",
        "              dy = np.array([dy])\n",
        "            F, W, H = dy.shape\n",
        "\n",
        "            for f in range(F):\n",
        "                for w in range(0, W-self.K, stride):\n",
        "                    for h in range(0, H-self.K, stride):\n",
        "                        dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
        "                        dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]\n",
        "\n",
        "            for f in range(F):\n",
        "                db[f] = np.sum(dy[f, :, :])\n",
        "\n",
        "            self.weights -= learning_rate * dw\n",
        "            self.bias -= learning_rate * db\n",
        "\n",
        "            return dx\n",
        "    \n",
        "    class POOL():\n",
        "        def __init__(self, pool_size):\n",
        "            self.pool = pool_size\n",
        "        \n",
        "        def forward(self, data, stride):\n",
        "            self.inputs = data\n",
        "            return util.AvgPool(data,self.pool,stride)\n",
        "        # change later\n",
        "        def backward(self, dy):\n",
        "            C, W, H = self.inputs.shape\n",
        "            dx = np.zeros(self.inputs.shape)\n",
        "            \n",
        "            for c in range(C):\n",
        "                for w in range(0, W, self.pool):\n",
        "                    for h in range(0, H, self.pool):\n",
        "                        st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
        "                        (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
        "                        dx[c, w+idx, h+idy] = dy[c, w//self.pool, h//self.pool]\n",
        "            return dx\n",
        "    \n",
        "    class DENSE():\n",
        "        def __init__(self, num_inputs, num_outputs, act):\n",
        "            self.weights = 2*(np.random.rand(num_inputs, num_outputs)-0.5)\n",
        "            self.bias = 2*(np.random.rand(num_outputs, 1)-0.5)\n",
        "            self.act=act\n",
        "        \n",
        "        def forward(self, data):\n",
        "            self.inputs = data\n",
        "            if self.act == 'tanh':\n",
        "              return util.Tanh(util.vanilla(data,self.weights,self.bias))\n",
        "            elif self.act == 'softmax':\n",
        "              self.out = util.softmax(util.vanilla(data,self.weights,self.bias)[:,0])\n",
        "              return self.out\n",
        "\n",
        "        def backward(self, dy, learning_rate):\n",
        "            # print(\"blah\",dy.shape)\n",
        "            if self.act == 'tanh':\n",
        "                dy = util.backTanh(dy).T\n",
        "                dw = dy.dot(self.inputs.T)\n",
        "                db = np.sum(dy, axis=1, keepdims=True)\n",
        "                dx = np.dot(dy.T, self.weights.T)\n",
        "            elif self.act=='softmax':\n",
        "                # print(\"bleh\",(self.out.T - dy.reshape(dy.shape[0],1)).shape)\n",
        "                # print(self.out.T)\n",
        "                # print(dy.reshape(dy.shape[0],1))\n",
        "                # print(self.out.T - dy.reshape(dy.shape[0],1))\n",
        "                # dy = self.out.T - dy.reshape(dy.shape[0],1)\n",
        "                dy = self.out.T.reshape(dy.shape[0],1) - dy.reshape(dy.shape[0],1)\n",
        "                dw = dy.dot(self.inputs.T)\n",
        "                db = np.sum(dy, axis=1, keepdims=True)\n",
        "                dx = np.dot(dy.T, self.weights.T)\n",
        "\n",
        "            # print(\"blah\",dy.shape)\n",
        "            # if dy.shape[0] == self.inputs.shape[0]:\n",
        "            #     dy = dy.T\n",
        "            # print(\"blah\",dy.shape)\n",
        "            # dw = dy.dot(self.inputs)\n",
        "            # db = np.sum(dy, axis=1, keepdims=True)\n",
        "            # dx = np.dot(dy.T, self.weights.T)\n",
        "\n",
        "            self.weights -= learning_rate * dw.T\n",
        "            self.bias -= learning_rate * db\n",
        "\n",
        "            return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "s27Nd1F1Lv8n"
      },
      "outputs": [],
      "source": [
        "def store_pickle(data, loc):\n",
        "    try:\n",
        "        pk.dump(data, open(loc, \"wb\"))\n",
        "        return 0\n",
        "        \n",
        "    except Exception as e:\n",
        "        return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tjeAHpj-LAjJ"
      },
      "outputs": [],
      "source": [
        "d = {0: [1, 1, 1, 0, 0, 0],\n",
        " 1: [0, 1, 1, 1, 0, 0],\n",
        " 2: [0, 0, 1, 1, 1, 0],\n",
        " 3: [0, 0, 0, 1, 1, 1],\n",
        " 4: [1, 0, 0, 0, 1, 1],\n",
        " 5: [1, 1, 0, 0, 0, 1],\n",
        " 6: [1, 1, 1, 1, 0, 0],\n",
        " 7: [0, 1, 1, 1, 1, 0],\n",
        " 8: [0, 0, 1, 1, 1, 1],\n",
        " 9: [1, 0, 0, 1, 1, 1],\n",
        " 10: [1, 1, 0, 0, 1, 1],\n",
        " 11: [1, 1, 1, 0, 0, 1],\n",
        " 12: [1, 1, 0, 1, 1, 0],\n",
        " 13: [0, 1, 1, 0, 1, 1],\n",
        " 14: [1, 0, 1, 1, 0, 1],\n",
        " 15: [1, 1, 1, 1, 1, 1]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os0w3q8CRmeO",
        "outputId": "8072a6f0-7c55-4300-88b6-bc09e44be500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  ' + str(test_X.shape))\n",
        "print('Y_test:  ' + str(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uyIcr3sbUaGb"
      },
      "outputs": [],
      "source": [
        "layers = Layers()\n",
        "C1 = layers.CONV(1,6,5)\n",
        "S2 = layers.POOL(2)\n",
        "C3 = layers.CONV(6,16,5)\n",
        "S4 = layers.POOL(2)\n",
        "C5 = layers.CONV(16,120,5)\n",
        "F6 = layers.DENSE(120,84,'tanh')\n",
        "F7 = layers.DENSE(84,10,'softmax')\n",
        "def forward_pass(im):\n",
        "  fmap1 = util.Tanh(C1.forward(np.array([im]),2,1))\n",
        "  # print(\"C1 Done\", fmap1.shape)\n",
        "  fmap2 = S2.forward(fmap1,2)\n",
        "  # print(\"S2 Done\", fmap2.shape)\n",
        "  fmap3 = util.Tanh(C3.forward(fmap2,0,1))\n",
        "  # print(\"C3 Done\", fmap3.shape)\n",
        "  fmap4 = S4.forward(fmap3,2)\n",
        "  # print(\"S4 Done\", fmap4.shape)\n",
        "  fmap5 = util.Tanh(C5.forward(fmap4,0,1))\n",
        "  # print(\"C5 Done\", fmap5.shape)\n",
        "  fmap6 = F6.forward(fmap5[:,0])\n",
        "  # print(\"F6 Done\", fmap6.shape)\n",
        "  fmap7 = F7.forward(fmap6)\n",
        "  return [fmap1, fmap2, fmap3, fmap4, fmap5, fmap6, fmap7]\n",
        "\n",
        "\n",
        "all_layers = [C1, S2, C3, S4, C5, F6, F7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "at0mR2z-YiNk"
      },
      "outputs": [],
      "source": [
        "output = forward_pass(train_X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RkEnZcNk71Aj"
      },
      "outputs": [],
      "source": [
        "# print(output[-1].reshape(10,1),output[-1].reshape(10,1).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtnxZLdHyk4x"
      },
      "source": [
        "# Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lhFXhHBOQYNM"
      },
      "outputs": [],
      "source": [
        "def one_hot(x):\n",
        "    temp = np.zeros(10)\n",
        "    temp[x] = 1\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UxQdQdp7yooX"
      },
      "outputs": [],
      "source": [
        "def train(epochs):\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch:\",epoch)\n",
        "        for i, im in tqdm(enumerate(train_X[:2000])):\n",
        "            # if i%100==0:\n",
        "              # print(i, end=\" \")\n",
        "            label = one_hot(train_y[i])\n",
        "            fmaps = forward_pass(im)\n",
        "            t = F7.backward(fmaps[-1], learning_rate = 0.05)\n",
        "            t = F6.backward(t, learning_rate = 0.05)\n",
        "            t = C5.backward(t, stride = 1, learning_rate = 0.05)\n",
        "            t = S4.backward(t)\n",
        "            t = C3.backward(t, stride = 1, learning_rate = 0.05)\n",
        "            t = S2.backward(t)\n",
        "            t = C1.backward(t, stride = 1, learning_rate = 0.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P-i8DpY63yD",
        "outputId": "dab48dff-b16f-4a4d-ff09-76fc799eef8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "469it [01:45,  4.46it/s]"
          ]
        }
      ],
      "source": [
        "train(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW5K3NTwCiiI"
      },
      "outputs": [],
      "source": [
        "def test(ind, to_print=True):\n",
        "    val = forward_pass(train_X[ind])\n",
        "    if to_print:\n",
        "      print(\"True Label:\", train_y[ind], \"Predicted:\", np.argmax(val[-1]))\n",
        "    return train_y[ind], np.argmax(val[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmGDuoUrFbFf"
      },
      "outputs": [],
      "source": [
        "acc = 0\n",
        "for i in tqdm(range(1000,1500)):\n",
        "  x,y = test(i,to_print=False)\n",
        "  if x==y:\n",
        "    acc += 1\n",
        "acc/=500\n",
        "acc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LeNet-5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
