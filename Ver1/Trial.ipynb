{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMAI, Spring 2022\n",
    "## Project: LeNet Implementation w/o AutoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The structure of LeNet\n",
    "<!-- ![LeNet](Images/Lenet_struct.png \"Structure of LeNet compared to AlexNet\") -->\n",
    "<img src=\"Images/Lenet_struct.png\" width=\"400\" height=\"300\">\n",
    "\n",
    "The plan is to implement a main `layer` class that deals with building different layers of the given neural network. This class will use text input to build different types of layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class utils():\n",
    "#     def padder(data,pad_length):\n",
    "#         im = data.copy()\n",
    "#         m = im.shape[0]\n",
    "#         n = im.shape[1]\n",
    "#         M, N = m + 2*pad_length, n + 2*pad_length \n",
    "#         im2 = zeros((M,N))\n",
    "#         im2[pad_length:m+pad_length , pad_length:n+pad_length] = im[:,:]\n",
    "        \n",
    "#         return im2\n",
    "    \n",
    "#     def Conv2D(data,kernel,stride =1):\n",
    "        \n",
    "#         kernel_size = np.array(np.array(kernel).shape)\n",
    "#         im_size = np.array(np.array(im).shape)\n",
    "\n",
    "#         final_dim = (im_size - kernel_size)/(stride) + np.array([1,1])\n",
    "#         final_im = np.zeros(tuple(final_dim))\n",
    "\n",
    "#         for i in range(final_dim[0]):\n",
    "#             for j in range(final_dim[1]):\n",
    "#                 final_im[i][j]= np.sum(np.multiply(im[i:i+kernel_size[0],j:j+kernel_size[1]],kernel))\n",
    "#         return final_im\n",
    "\n",
    "#     def AvgPool(self,data,kern_size,stride):\n",
    "#         kern = np.ones(kern_size)/(kern_size)\n",
    "#         return self.Conv2D(data,kern,stride)\n",
    "\n",
    "#     def softmax(x):\n",
    "#         denom = np.sum(np.exp(x))\n",
    "#         y = x/denom;\n",
    "#         return y\n",
    "\n",
    "#     def sigmoid(x):\n",
    "#         return 1/(1+np.exp(-x))\n",
    "\n",
    "#     def Tanh(x):\n",
    "#         a = 1.7159 \n",
    "#         s = 2/3\n",
    "#         return a*np.tanh(s*x)\n",
    "    \n",
    "#     def ReLU(x):\n",
    "#         return np.max(x,0)\n",
    "    \n",
    "#     def normal(self, data):\n",
    "#         padded = self.padder(data,2)\n",
    "    \n",
    "#     def vanilla(data,weights,bias, activation = 'Tanh'):\n",
    "#         if activation == 'Tanh':\n",
    "#             return utils.Tanh(np.dot(weights,data) + bias)\n",
    "#         elif activation == 'sigmoid'\n",
    "#             return  utils.sigmoid(np.dot(weights,data) + bias)\n",
    "\n",
    "class utils():\n",
    "    def padder(self, data, pad_length):\n",
    "        im = data.copy()\n",
    "        m = im.shape[0]\n",
    "        n = im.shape[1]\n",
    "        M, N = m + 2*pad_length, n + 2*pad_length \n",
    "        im2 = np.zeros((M,N))\n",
    "        im2[pad_length:m+pad_length , pad_length:n+pad_length] = im[:,:]\n",
    "        \n",
    "        return im2\n",
    "    \n",
    "    def Conv2D(self, inputs, weights, bias, padding, K, F, stride =1):\n",
    "        C, W, H = inputs.shape\n",
    "        WW = (W - K)//stride + 1\n",
    "        HH = (H - K)//stride + 1\n",
    "\n",
    "        feature_maps = np.zeros((F, WW, HH))\n",
    "\n",
    "        for f in range(F):\n",
    "            for w in range(WW):\n",
    "                for h in range(HH):\n",
    "                    # ic(f, w, h, K, weights[f, :, :, :].shape, bias[f].shape, inputs[:, w:w+K, h:h+K].shape)\n",
    "                    wi = w * stride\n",
    "                    wj = wi + K \n",
    "                    hi = h * stride\n",
    "                    hj = hi + K \n",
    "                    feature_maps[f,wi,hi]=np.sum(inputs[:,wi:wj,hi:hj]*weights[f,:,:,:])+bias[f]\n",
    "\n",
    "        return feature_maps\n",
    "\n",
    "    def AvgPool(self, data, pool_size, stride):\n",
    "        C, W, H = data.shape\n",
    "\n",
    "        new_width = (W - pool_size)//stride + 1\n",
    "        new_height = (H - pool_size)//stride + 1\n",
    "\n",
    "        out = np.zeros((C, new_width, new_height))\n",
    "\n",
    "        for c in range(C):\n",
    "            for w in range(new_width):\n",
    "                for h in range(new_height):\n",
    "                    out[c, w, h] = np.mean(data[c, w*stride:w*stride+pool_size, h*stride:h*stride+pool_size])\n",
    "  \n",
    "        return out\n",
    "\n",
    "    def softmax(self, x):\n",
    "        denom = np.sum(np.exp(x))\n",
    "        y = np.exp(x)/denom;\n",
    "        return y\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def Tanh(self, x):\n",
    "        a = 1.7159 \n",
    "        s = 2/3\n",
    "        return a*np.tanh(s*x)\n",
    "\n",
    "    def backTanh(self, x):\n",
    "        a = 1.7159 \n",
    "        s = 2/3\n",
    "        return a*s*(1-np.tanh(s*x)**2)\n",
    "    \n",
    "    def ReLU(self, x):\n",
    "        return np.max(x,0)\n",
    "    \n",
    "    def normal(self, data):\n",
    "        padded = self.padder(data,2)\n",
    "    \n",
    "    def vanilla(self, data, weights, bias, activation = 'Tanh'):\n",
    "        if activation == 'Tanh':\n",
    "            # ic(np.transpose(weights).shape, data[:,0].shape)\n",
    "            # ic(data)\n",
    "            return self.Tanh(np.dot(np.transpose(weights),data) + bias)\n",
    "        elif activation == 'sigmoid':\n",
    "            return  self.sigmoid(np.dot(np.transpose(weights),data) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "util = utils()\n",
    "class Layers():\n",
    "    class CONV():\n",
    "        def __init__(self, inputs_channel, num_filters, kernel_size):\n",
    "          self.F = num_filters\n",
    "          self.K = kernel_size\n",
    "          self.C = inputs_channel\n",
    "\n",
    "          self.weights = 2*(np.random.rand(self.F,self.C, self.K, self.K)-0.5)\n",
    "          self.bias = 2*(np.random.rand(self.F, 1)-0.5)\n",
    "\n",
    "        def forward(self, inputs, padding, stride):\n",
    "            C = inputs.shape[0]\n",
    "            W = inputs.shape[1]+2*padding\n",
    "            H = inputs.shape[2]+2*padding\n",
    "\n",
    "            self.inputs = np.zeros((C, W, H))\n",
    "\n",
    "            for c in range(inputs.shape[0]):\n",
    "                self.inputs[c,:,:] = util.padder(inputs[c,:,:], padding)\n",
    "\n",
    "            return util.Conv2D(self.inputs, self.weights, self.bias, padding, self.K, self.F, stride)\n",
    "          \n",
    "        def backward(self, dy, stride, learning_rate):\n",
    "            dy = util.backTanh(dy)\n",
    "\n",
    "            C, W, H = self.inputs.shape\n",
    "\n",
    "            dx = np.zeros(self.inputs.shape)\n",
    "            dw = np.zeros(self.weights.shape)\n",
    "            db = np.zeros(self.bias.shape)\n",
    "\n",
    "            if len(dy.shape)==2:\n",
    "              dy = np.array([dy])\n",
    "            F, W, H = dy.shape\n",
    "\n",
    "            for f in range(F):\n",
    "                for w in range(0, W-self.K, stride):\n",
    "                    for h in range(0, H-self.K, stride):\n",
    "                        dw[f,:,:,:]+=dy[f,w,h]*self.inputs[:,w:w+self.K,h:h+self.K]\n",
    "                        dx[:,w:w+self.K,h:h+self.K]+=dy[f,w,h]*self.weights[f,:,:,:]\n",
    "\n",
    "            for f in range(F):\n",
    "                db[f] = np.sum(dy[f, :, :])\n",
    "\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "            return dx\n",
    "    \n",
    "    class POOL():\n",
    "        def __init__(self, pool_size):\n",
    "            self.pool = pool_size\n",
    "        \n",
    "        def forward(self, data, stride):\n",
    "            self.inputs = data\n",
    "            return util.AvgPool(data,self.pool,stride)\n",
    "        # change later\n",
    "        def backward(self, dy):\n",
    "            C, W, H = self.inputs.shape\n",
    "            dx = np.zeros(self.inputs.shape)\n",
    "            \n",
    "            for c in range(C):\n",
    "                for w in range(0, W, self.pool):\n",
    "                    for h in range(0, H, self.pool):\n",
    "                        st = np.argmax(self.inputs[c,w:w+self.pool,h:h+self.pool])\n",
    "                        (idx, idy) = np.unravel_index(st, (self.pool, self.pool))\n",
    "                        dx[c, w+idx, h+idy] = dy[c, w//self.pool, h//self.pool]\n",
    "            return dx\n",
    "    \n",
    "    class DENSE():\n",
    "        def __init__(self, num_inputs, num_outputs, act):\n",
    "            self.weights = 2*(np.random.rand(num_inputs, num_outputs)-0.5)\n",
    "            self.bias = 2*(np.random.rand(num_outputs, 1)-0.5)\n",
    "            self.act=act\n",
    "        \n",
    "        def forward(self, data):\n",
    "            self.inputs = data\n",
    "            if self.act == 'tanh':\n",
    "              return util.Tanh(util.vanilla(data,self.weights,self.bias))\n",
    "            elif self.act == 'softmax':\n",
    "              self.out = util.softmax(util.vanilla(data,self.weights,self.bias)[:,0])\n",
    "              return self.out\n",
    "\n",
    "        def backward(self, dy, learning_rate):\n",
    "            # print(\"blah\",dy.shape)\n",
    "            if self.act == 'tanh':\n",
    "                dy = util.backTanh(dy).T\n",
    "                dw = dy.dot(self.inputs.T)\n",
    "                db = np.sum(dy, axis=1, keepdims=True)\n",
    "                dx = np.dot(dy.T, self.weights.T)\n",
    "            elif self.act=='softmax':\n",
    "                # print(\"bleh\",(self.out.T - dy.reshape(dy.shape[0],1)).shape)\n",
    "                # print(self.out.T)\n",
    "                # print(dy.reshape(dy.shape[0],1))\n",
    "                # print(self.out.T - dy.reshape(dy.shape[0],1))\n",
    "                # dy = self.out.T - dy.reshape(dy.shape[0],1)\n",
    "                dy = self.out.T.reshape(dy.shape[0],1) - dy.reshape(dy.shape[0],1)\n",
    "                dw = dy.dot(self.inputs.T)\n",
    "                db = np.sum(dy, axis=1, keepdims=True)\n",
    "                dx = np.dot(dy.T, self.weights.T)\n",
    "\n",
    "            # print(\"blah\",dy.shape)\n",
    "            # if dy.shape[0] == self.inputs.shape[0]:\n",
    "            #     dy = dy.T\n",
    "            # print(\"blah\",dy.shape)\n",
    "            # dw = dy.dot(self.inputs)\n",
    "            # db = np.sum(dy, axis=1, keepdims=True)\n",
    "            # dx = np.dot(dy.T, self.weights.T)\n",
    "\n",
    "            self.weights -= learning_rate * dw.T\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "            return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f67c674820>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = (mnist.test_images())\n",
    "imshow(255- t[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avg_k_sq(im,k):\n",
    "    m = int((k-1)/2)\n",
    "    \n",
    "    M = im.shape[0]\n",
    "    N = im.shape[1]     \n",
    "\n",
    "    Paddy = zeros(( M + 2*m, N + 2*m))\n",
    "   \n",
    "    Paddy[m:M+m,m:N+m] = im[:,:]\n",
    "\n",
    "    im2 = im.copy()\n",
    "    filt = (1/(k*k))*ones((k,k))\n",
    "\n",
    "    for i in range(im.shape[0]):\n",
    "        for j in range(im.shape[1]):\n",
    "            im2[i][j] = round(sum(Paddy[i:i+k,j:j+k]*filt))\n",
    "            \n",
    "    return im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'inputs_channel', 'num_filters', and 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ASHUTH~1\\AppData\\Local\\Temp/ipykernel_16236/904552268.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'inputs_channel', 'num_filters', and 'kernel_size'"
     ]
    }
   ],
   "source": [
    "A = Layers.CONV(1,1,5)\n",
    "imshow(A.forward())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0daff1dc3e0c9309a682d6632bce09174730adaee728d6e225f20579eba431a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
